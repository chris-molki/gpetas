{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645f35c6-6e08-4b7b-b0a4-f971fc14fcf1",
   "metadata": {},
   "source": [
    "# Loading required packages/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e32cde6-7e16-4ac3-ad38-d16b3145e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gpetas\n",
    "import numpy as np\n",
    "import datetime\n",
    "time_format = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "# auto reload using developer version\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29175ed1-86ed-45ba-b796-ab489349d114",
   "metadata": {},
   "source": [
    "# 1) Define domain\n",
    "One has to specify following aspects of the domain of the data:\n",
    "\n",
    "* time domain\n",
    "    * time borders for the training T_borders_training\n",
    "    * time borders for the testing T_borders_testing\n",
    "    * time origin (when was '0.' days) given by a *datetime* format: '%Y-%m-%d %H:%M:%S.%f'\n",
    "* spatial domain X_borders\n",
    "* domain of the marks (magnitudes) usually by m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf3583a0-768f-4c75-bc68-3b6ab5c6a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T_borders_all': array([   0., 4383.]),\n",
       " 'T_borders_training': array([   0., 3000.]),\n",
       " 'T_borders_testing': array([3000., 4383.]),\n",
       " 'X_borders': array([[-120., -113.],\n",
       "        [  30.,   37.]]),\n",
       " 'X_borders_UTM_km': None,\n",
       " 'X_borders_original': None,\n",
       " 'time_origin': '2010-01-01 00:00:00.0',\n",
       " 'm0': 3.5,\n",
       " 'case_name': 'Rxxx'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=========================================\n",
    "#=========================================\n",
    "### (1) Setup domain obj\n",
    "#=========================================\n",
    "#=========================================\n",
    "\n",
    "\n",
    "\n",
    "#=========================================\n",
    "# 1a) time domain\n",
    "#=========================================\n",
    "time_origin = '2010-01-01 00:00:00.0'\n",
    "time_end = '2022-01-01 00:00:00.0'\n",
    "T_borders_training = np.array([0.,3000.]) # in days\n",
    "\n",
    "time_origin_obj = datetime.datetime.strptime(time_origin, time_format).replace(tzinfo=datetime.timezone.utc)\n",
    "time_end_obj = datetime.datetime.strptime(time_end, time_format).replace(tzinfo=datetime.timezone.utc)\n",
    "delta_Tall=(time_end_obj-time_origin_obj).total_seconds()/(60.*60.*24)\n",
    "T_borders_all = np.array([0.,delta_Tall])\n",
    "if T_borders_training[1]>delta_Tall:\n",
    "    T_borders_training[1] = 0.5*delta_Tall\n",
    "    print('T_borders_training[1] reset to=',T_borders_training[1])\n",
    "\n",
    "#=========================================\n",
    "# 1b) spatial domain\n",
    "#=========================================\n",
    "X_borders = np.array([[-120., -113.],[  30.,   37.]]) # [[xrange longitudes],[yrange latitudes]] in degrees\n",
    "\n",
    "#=========================================\n",
    "# mark domain: [m0,+inf)\n",
    "#=========================================\n",
    "m0=3.5\n",
    "\n",
    "\n",
    "#=========================================\n",
    "# 1c) generate domain_obj\n",
    "#=========================================\n",
    "domain_obj = gpetas.domain_setup.domain_class()\n",
    "domain_obj.T_borders_all = T_borders_all\n",
    "domain_obj.T_borders_training=T_borders_training\n",
    "domain_obj.T_borders_testing = np.array([T_borders_training[1],T_borders_all[1]])\n",
    "domain_obj.time_origin = time_origin\n",
    "domain_obj.X_borders = X_borders\n",
    "domain_obj.m0 = m0\n",
    "# case_name (optional)\n",
    "# domain_obj.case_name = case_name\n",
    "vars(domain_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82135b-9213-4e29-a350-706e3b688b75",
   "metadata": {},
   "source": [
    "# 2) Data from an online source using pycsep\n",
    "Data can be easily downloaded from online sources as\n",
    "* USGS: HOST = 'earthquake.usgs.gov' for California\n",
    "\n",
    "Here we use functionalities provided by the ``pycsep`` package which \n",
    "facilitates the data access.\n",
    "\n",
    "``pycsep`` provides access to the \n",
    "ComCat web API and to the \n",
    "Bollettino Sismico Italiano API using\n",
    "* csep.query_comcat()\n",
    "* csep.query_bsi().\n",
    "\n",
    "Make sure that pycsep is installed, otherwise install pycsep package with the \n",
    "``gpetas_env`` activated:\n",
    "```python\n",
    "conda activate gpetas_env\n",
    "conda install --channel conda-forge pycsep\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e031f37-f76b-4438-adbd-c38c34502efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'csep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#=========================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#=========================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m### (2) Download data and generate data_object\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# get pycsep catalog object\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# install pycsep first using conda: conda install --channel conda-forge pycsep\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsep\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcsep\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m comcat\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#=========================================\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2a) get data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#=========================================\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'csep'"
     ]
    }
   ],
   "source": [
    "#=========================================\n",
    "#=========================================\n",
    "### (2) Download data and generate data_object\n",
    "#=========================================\n",
    "#=========================================\n",
    "# generate data_obj with download data from online comcat source using pycsep\n",
    "# get pycsep catalog object\n",
    "# install pycsep first using conda: conda install --channel conda-forge pycsep\n",
    "import csep\n",
    "from csep.utils import comcat\n",
    "\n",
    "#=========================================\n",
    "# 2a) get data\n",
    "#=========================================\n",
    "start_time = datetime.datetime.strptime(domain_obj.time_origin, time_format).replace(tzinfo=datetime.timezone.utc)\n",
    "end_time = start_time + datetime.timedelta(days=np.diff(domain_obj.T_borders_all).item())\n",
    "min_magnitude=domain_obj.m0\n",
    "min_latitude=domain_obj.X_borders[1,0]\n",
    "max_latitude=domain_obj.X_borders[1,1]\n",
    "min_longitude=domain_obj.X_borders[0,0]\n",
    "max_longitude=domain_obj.X_borders[0,1]\n",
    "catalog_obj = csep.query_comcat(start_time=start_time, end_time=end_time, \n",
    "                        min_magnitude=min_magnitude, \n",
    "                        min_latitude=min_latitude,max_latitude=max_latitude, \n",
    "                        min_longitude=min_longitude, max_longitude=max_longitude)\n",
    "\n",
    "#=========================================\n",
    "# 2b) generate gpetas data_object and saves is it into './output/inference_results/data_obj_Rxxx.all'\n",
    "#=========================================\n",
    "data_obj = gpetas.utils.get_data_pycsep.data_obj__from_catalog_obj(catalog_obj=catalog_obj,R_obj=domain_obj)\n",
    "h=gpetas.plotting.plot_setting(data_obj=data_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88630e-5984-426a-9cc8-581568473940",
   "metadata": {},
   "source": [
    "# 3) Inference setup\n",
    "In order to perform inference (Bayesian or Maximum Likelihood)\n",
    "one needs to define or setup several auxiliary variables.\n",
    "This is done by creating a ***setup_obj*** for the inference which \n",
    "includes all required information of the\n",
    "* Bayesian inference, i.e., Gibbs sampling procedure with Gaussian process \n",
    "modelling of the background intensity (GP-ETAS, gpetas) or\n",
    "* classical Maximum Likelihood estimation (MLE) using a kernel density estimator \n",
    "for the background intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4d486-e927-49a2-a2f1-f82eb2a55130",
   "metadata": {},
   "source": [
    "## 3.1) Setup object for Bayesian inference (Gibbs sampler)\n",
    "Sampling the posterior requires a\n",
    "*setup_obj*, which contains everything\n",
    "one needs for inference: data, domain definition, setup variables (Gibbs sampling parameters).\n",
    "\n",
    "The inference procedure requires the following:\n",
    "* Specification of the ***domain*** of the analysis specified in *domain_obj* which is included in the *data_obj*\n",
    "* ***Data*** stored in a *data_obj* which includes *domain_obj*\n",
    "* ***parameters of the Gibbs sampler***, e.g., burn-in, number of posterior samples, start values etc. saved in *setup_obj*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "817e054d-c271-4ba5-8feb-aee62796dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nmax= [15187.52797552]\n",
      "#iters 101\n",
      "Output subdirectory exists\n",
      "setup_obj has been created and saved: ./output/inference_results/setup_obj_Rxxx.all\n"
     ]
    }
   ],
   "source": [
    "#=========================================\n",
    "#=========================================\n",
    "### (3.1) Setup object of the Gibbs sampler\n",
    "#=========================================\n",
    "#=========================================\n",
    "\n",
    "\n",
    "#=========================================\n",
    "# 3.1a) Loading data_obj which includes\n",
    "#                        domain_obj\n",
    "#=========================================\n",
    "case_name = 'Rxxx'\n",
    "fname = './output/inference_results/data_obj_%s.all'%(case_name) #prevously in 2) generated and saved\n",
    "data_obj = np.load(fname,allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "#=========================================\n",
    "# 3.1b) variables of the Gibbs sampler\n",
    "#=========================================\n",
    "# sampler parameters\n",
    "burnin = 10                               # number of discared initial samples. default: 5000\n",
    "Ksamples = 20                             # number of samples of the joint posterior default: 500 (a few hundreds)\n",
    "thinning = 5                              # default:10 # or 20:thinning of the obtained samples in order to avoid autocorrelation\n",
    "MH_proposals_offspring = 100               # Number of MH proposals for offspring params\n",
    "MH_cov_empirical_yes = None                # using empirical cov for proposal distribution\n",
    "sigma_proposal_offspring_params = None     # uses default values: 0.01**2 # alternatives:0.03**2\n",
    "kth_sample_obj = None                      # starting sampling from kth sample \n",
    "num_iterations = Ksamples*thinning+1\n",
    "print('#iters',num_iterations)\n",
    "\n",
    "\n",
    "# offspring\n",
    "prior_theta_dist = 'gamma'                 # specifies prior distribution either 'gamma' or 'uniform'\n",
    "prior_theta_params = None\n",
    "theta_start_Kcpadgq = None                 # uses default values:\n",
    "spatial_offspring = 'R'                    # alternatives: 'G' gaussian \n",
    "stable_theta_sampling = None #'yes'              # constraint on theta that only stable Hawkes processes are allowed\n",
    "\n",
    "\n",
    "# background: \n",
    "cov_params = None                          # start values of hypers, uses default: silverman rule\n",
    "mu_nu0 = None                              # mean of hyper prior on nu_0, uses default value:\n",
    "\n",
    "\n",
    "\n",
    "# background: spatial resolution for plotting/evaluations\n",
    "ngrid_per_dim = 50                         # default value: 50 corresponds to a 50x50 grid over X domain\n",
    "X_grid = gpetas.some_fun.make_X_grid(data_obj.domain.X_borders, nbins=ngrid_per_dim)\n",
    "                                           # generates spatial grid for plotting etc.\n",
    "    \n",
    "# general \n",
    "time_origin = data_obj.domain.time_origin\n",
    "case_name = data_obj.case_name\n",
    "    \n",
    "# save results\n",
    "output_dir = './output/inference_results'\n",
    "outdir = output_dir\n",
    "\n",
    "\n",
    "# Generating GS sampler setup_obj\n",
    "setup_obj = gpetas.setup_Gibbs_sampler.setup_sampler(data_obj=data_obj,\n",
    "             utm_yes=None,\n",
    "             spatial_offspring=spatial_offspring,\n",
    "             theta_start_Kcpadgq=theta_start_Kcpadgq,\n",
    "             sigma_proposal_offspring_params=sigma_proposal_offspring_params,\n",
    "             ngrid_per_dim=ngrid_per_dim,\n",
    "             cov_params=cov_params,\n",
    "             mu_nu0=None,\n",
    "             X_grid=X_grid,\n",
    "             outdir=outdir,\n",
    "             prior_theta_dist=prior_theta_dist,\n",
    "             prior_theta_params=prior_theta_params,\n",
    "             stable_theta_sampling=stable_theta_sampling,\n",
    "             time_origin=time_origin,\n",
    "             case_name=case_name,\n",
    "             burnin=burnin, \n",
    "             Ksamples=Ksamples,\n",
    "             num_iterations=num_iterations,\n",
    "             thinning=thinning,\n",
    "             MH_proposals_offspring=MH_proposals_offspring,\n",
    "             MH_cov_empirical_yes=MH_cov_empirical_yes,\n",
    "             kth_sample_obj=kth_sample_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22d283-8ce1-4310-94d8-4f03d004025e",
   "metadata": {},
   "source": [
    "# 4) Inference: gpetas and MLE\n",
    "Inference can be done ***in the terminal*** using ``demo_inference_setup_obj_v008.py`` \n",
    "program.\n",
    "\n",
    "This program does ***Gibbs sampling and MLE*** automatically using \n",
    "the setup of the Gibbs sampler also for MLE.\n",
    "\n",
    "The ``demo_inference_setup_obj_v008.py`` requires only one argument, namely \n",
    "the *setup_obj file*.\n",
    "\n",
    "In the terminal type:\n",
    "\n",
    "```python\n",
    "conda activate gpetas_env\n",
    "python3.9 demo_inference_setup_obj_v008.py --fsetup output/inference_results/setup_obj_Rxxx.all\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c0095-4d23-4f3b-82a7-8e824db2730b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
